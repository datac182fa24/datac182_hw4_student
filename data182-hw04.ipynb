{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohhBQGEKZ9z-"
   },
   "source": [
    "# Vision Transformer and Masked Autoencoder\n",
    "\n",
    "In this assignment, you will be implementing [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) and [Masked Autoencoder (MAE)](https://arxiv.org/abs/2111.06377)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T01:57:01.369730Z",
     "iopub.status.busy": "2024-09-18T01:57:01.369451Z",
     "iopub.status.idle": "2024-09-18T01:57:01.377217Z",
     "shell.execute_reply": "2024-09-18T01:57:01.375778Z",
     "shell.execute_reply.started": "2024-09-18T01:57:01.369698Z"
    },
    "id": "vx2F48YVt47m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr_tSqTXc0VE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-f8SLhqa585"
   },
   "source": [
    "While we recommend working on Colab with GPU enabled, for practical reasons (namely limited Colab GPU availability), our advice is to first complete this assignment in a CPU environment with small model sizes, then re-run the notebook in GPU mode with \"full\" model sizes\".\n",
    "\n",
    "To help you with this, in all Colab cells where you train ML models, we have provided two model settings: \"ModelSize.FULL\" and \"ModelSize.TINY\". We recommend first running \"ModelSize.TINY\" in a CPU runtime, and get your notebook to first successfully run end-to-end in a CPU environment. Note that, due to the smaller model sizes, it's unlikely you will hit the accuracy checks.\n",
    "\n",
    "Once you're convinced that your implementations are correct, then switch the model train mode to use larger model sizes (eg `MODEL_SIZE = ModelSize.FULL`), switch to the GPU environment, and run the entire notebook (eg \"Runtime->Run All\" or \"Runtime->Run cell and below\").\n",
    "\n",
    "Tip: to further reduce time, feel free to reduce number of train epochs as well: for initial checks, 5 epochs is probably good enough. Just remember to set epochs back to the full value for your final submission!\n",
    "\n",
    "**Important**: to receive full credit on this homework (eg pass all autograder tests), you will need to submit a submission using the \"ModelSize.FULL\" model settings.\n",
    "\n",
    "Note that this notebook is designed to work both in a CPU and GPU runtime.\n",
    "\n",
    "To switch to a GPU runtime, click the dropdown menu right of the \"Connect\" button on the upper-right corner of the Colab UI, choose \"Change runtime type\", and choose \"T4 GPU\". This will connect you to an instance with an Nvidia GPU.\n",
    "\n",
    "Here is a guide that shows how to change the Colab notebook runtime to the T4 GPU: [link](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/)\n",
    "\n",
    "The dependencies will be installed once the notebooks are excuted.\n",
    "\n",
    "You should make a copy of this notebook to your Google Drive otherwise the outputs will not be saved.\n",
    "Once the folder is copied, you can start working by clicking a Jupyter Notebook and openning it in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ModelSize(str, Enum):\n",
    "    # full: use the full-sized model architectures.\n",
    "    #   Recommended to use this only for GPU runtimes.\n",
    "    FULL = \"full\"\n",
    "    # tiny: use tiny model architectures.\n",
    "    #   Recommended for intial development in CPU runtimes.\n",
    "    TINY = \"tiny\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: this global var controls the model size to use for this notebook.\n",
    "# Advice: for initial development, set this to ModelSize.TINY and use a CPU runtime.\n",
    "#   Once you're able to run this notebook end-to-end on a CPU and things seem to work\n",
    "#   (but with lower accuracy due to weaker model), try switching to a GPU runtime,\n",
    "#   then switch to ModelSize.FULL.\n",
    "MODEL_SIZE = ModelSize.FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:03.233848Z",
     "iopub.status.busy": "2024-09-17T04:54:03.233143Z",
     "iopub.status.idle": "2024-09-17T04:54:04.282197Z",
     "shell.execute_reply": "2024-09-17T04:54:04.280989Z",
     "shell.execute_reply.started": "2024-09-17T04:54:03.233788Z"
    },
    "id": "nI_3cNKMGL2a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi # Confirm GPU is enabled (only if you intend to use a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:04.285108Z",
     "iopub.status.busy": "2024-09-17T04:54:04.284670Z",
     "iopub.status.idle": "2024-09-17T04:54:18.298312Z",
     "shell.execute_reply": "2024-09-17T04:54:18.296976Z",
     "shell.execute_reply.started": "2024-09-17T04:54:04.285063Z"
    },
    "id": "9hFl82FA59le",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:18.300324Z",
     "iopub.status.busy": "2024-09-17T04:54:18.299914Z",
     "iopub.status.idle": "2024-09-17T04:54:23.880346Z",
     "shell.execute_reply": "2024-09-17T04:54:23.879343Z",
     "shell.execute_reply.started": "2024-09-17T04:54:18.300278Z"
    },
    "id": "imL8NunGXV5f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import einops\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:24.874830Z",
     "iopub.status.busy": "2024-09-17T04:54:24.874509Z",
     "iopub.status.idle": "2024-09-17T04:54:24.880201Z",
     "shell.execute_reply": "2024-09-17T04:54:24.879250Z",
     "shell.execute_reply.started": "2024-09-17T04:54:24.874798Z"
    },
    "id": "hHjvJvZhkv-i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# IMPORTANT: change to your own root folder path! This dir should contain the contents of:\n",
    "# https://github.com/datac182fa24/datac182_hw4_student\n",
    "root_folder = \"/content/drive/MyDrive/data_c182_fa2024/homeworks/datac182_hw4_student/\"\n",
    "os.chdir(root_folder)\n",
    "# validate that we're in the right directory (ls should show files like: autograder_student.pt, data182-hw04.ipynb, etc )\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1wxozGQcVUz"
   },
   "source": [
    "**Note**: change ```root_folder``` to the folder of this notebook in your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:24.891084Z",
     "iopub.status.busy": "2024-09-17T04:54:24.890639Z",
     "iopub.status.idle": "2024-09-17T04:54:30.417213Z",
     "shell.execute_reply": "2024-09-17T04:54:30.416248Z",
     "shell.execute_reply.started": "2024-09-17T04:54:24.891051Z"
    },
    "id": "nl3t1xKoWee7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Load Testing Data\n",
    "os.makedirs(\"./content\", exist_ok=True)\n",
    "test_data = torch.load('./test_reference.pt', weights_only=True)\n",
    "auto_grader_data = torch.load('./autograder_student.pt', weights_only=True)\n",
    "auto_grader_data['output'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:30.427302Z",
     "iopub.status.busy": "2024-09-17T04:54:30.426418Z",
     "iopub.status.idle": "2024-09-17T04:54:30.438094Z",
     "shell.execute_reply": "2024-09-17T04:54:30.437210Z",
     "shell.execute_reply.started": "2024-09-17T04:54:30.427259Z"
    },
    "id": "Uv1eceRfsIHZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Utilities for Testing\n",
    "def save_auto_grader_data():\n",
    "    torch.save(\n",
    "        {'output': auto_grader_data['output']},\n",
    "        'autograder.pt'\n",
    "    )\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return torch.max(\n",
    "        torch.abs(x - y)\n",
    "        / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n",
    "    ).item()\n",
    "\n",
    "def check_error(name, x, y, tol=1e-3):\n",
    "    error = rel_error(x, y)\n",
    "    if error > tol:\n",
    "        print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n",
    "    else:\n",
    "        print(f'The relative error for {name} is {error}')\n",
    "\n",
    "def check_acc(acc, threshold):\n",
    "    if acc < threshold:\n",
    "        print(f'The accuracy {acc} should >= threshold accuracy {threshold}')\n",
    "    else:\n",
    "        print(f'The accuracy {acc} is better than threshold accuracy {threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ii0DgIQdsmQ"
   },
   "source": [
    "## Vision Transformer\n",
    "The first part of this notebook is implementing Vision Transformer (ViT) and training it on CIFAR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MquneePhckk8"
   },
   "source": [
    "### Image patchify and unpatchify\n",
    "\n",
    "In ViT, an image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. The architecture can be seen in the following figure.\n",
    "![vit](https://github.com/google-research/vision_transformer/blob/main/vit_figure.png?raw=true)\n",
    "\n",
    "To get started with implementing ViT, you need to implement splitting image batch into fixed-size patches batch in ```patchify``` and combining patches batch into the original image batch in ```unpatchify```.\n",
    "\n",
    "We strongly recommend using [einops](https://github.com/arogozhnikov/einops) for flexible tensor operations, you can check out its [tutorial](https://einops.rocks/1-einops-basics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:30.442056Z",
     "iopub.status.busy": "2024-09-17T04:54:30.441440Z",
     "iopub.status.idle": "2024-09-17T04:54:30.451812Z",
     "shell.execute_reply": "2024-09-17T04:54:30.450911Z",
     "shell.execute_reply.started": "2024-09-17T04:54:30.442023Z"
    },
    "id": "hS2qs7nD5iyp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "def patchify(images, patch_size=4):\n",
    "    \"\"\"Splitting images into patches.\n",
    "    Args:\n",
    "        images: Input tensor with size (batch, channels, height, width)\n",
    "            We can assume that image is square where height == width.\n",
    "    Returns:\n",
    "        A batch of image patches with size (\n",
    "          batch, (height / patch_size) * (width / patch_size),\n",
    "        channels * patch_size * patch_size)\n",
    "    \"\"\"\n",
    "    batch, channels, height, width = images.shape\n",
    "    assert height == width, f\"Image height {height} should be equal to width {width}\"\n",
    "    assert height % patch_size == 0, f\"Image height {height} must be divisible by patch size {patch_size}\"\n",
    "    assert width % patch_size == 0, f\"Image width {width} must be divisible by patch size {patch_size}\"\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    # Hint: can be implemented in 1 line, use einops rearrange (https://einops.rocks/api/rearrange/)\n",
    "\n",
    "    # END YOUR CODE\n",
    "    assert patches.shape == (batch, (height // patch_size) * (width // patch_size),\n",
    "                             channels * patch_size * patch_size)\n",
    "\n",
    "    return patches\n",
    "\n",
    "def unpatchify(patches, patch_size=4):\n",
    "    \"\"\"Combining patches into images.\n",
    "    Args:\n",
    "        patches: Input tensor with size (\n",
    "        batch, (height / patch_size) * (width / patch_size),\n",
    "        channels * patch_size * patch_size)\n",
    "    Returns:\n",
    "        A batch of images with size (batch, channels, height, width)\n",
    "    \"\"\"\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    # Hint: can be implemented in 1 line, use einops rearrange (https://einops.rocks/api/rearrange/)\n",
    "\n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:30.453206Z",
     "iopub.status.busy": "2024-09-17T04:54:30.452948Z",
     "iopub.status.idle": "2024-09-17T04:54:30.518891Z",
     "shell.execute_reply": "2024-09-17T04:54:30.517862Z",
     "shell.execute_reply.started": "2024-09-17T04:54:30.453177Z"
    },
    "id": "plMY_EoNZ4ip",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation\n",
    "x = test_data['input']['patchify']\n",
    "y = test_data['output']['patchify']\n",
    "check_error('patchify', patchify(x), y)\n",
    "\n",
    "x = auto_grader_data['input']['patchify']\n",
    "auto_grader_data['output']['patchify'] = patchify(x)\n",
    "save_auto_grader_data()\n",
    "\n",
    "\n",
    "x = test_data['input']['unpatchify']\n",
    "y = test_data['output']['unpatchify']\n",
    "check_error('unpatchify', unpatchify(x), y)\n",
    "\n",
    "x = auto_grader_data['input']['unpatchify']\n",
    "auto_grader_data['output']['unpatchify'] = unpatchify(x)\n",
    "\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFlVruRYhBlR"
   },
   "source": [
    "### ViT model\n",
    "\n",
    "You need to use the given Transformer encoder ```Transformer``` to implement ViT in ```ViT``` and ```ClassificationViT```.\n",
    "\n",
    "Our implementation differs from the original in that we use global average pooling instead of a dedicated classifier token. This is recommended in Scaling Vision Transformers (2022) https://arxiv.org/pdf/2106.04560."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:30.520764Z",
     "iopub.status.busy": "2024-09-17T04:54:30.520415Z",
     "iopub.status.idle": "2024-09-17T04:54:30.539256Z",
     "shell.execute_reply": "2024-09-17T04:54:30.538320Z",
     "shell.execute_reply.started": "2024-09-17T04:54:30.520729Z"
    },
    "id": "gpZeS39N8BPJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer Encoder\n",
    "    Args:\n",
    "        embedding_dim: dimension of embedding\n",
    "        n_heads: number of attention heads\n",
    "        n_layers: number of attention layers\n",
    "        feedforward_dim: hidden dimension of MLP layer\n",
    "    Returns:\n",
    "        Transformer embedding of input\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=256, n_heads=4, n_layers=4, feedforward_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=self.n_heads,\n",
    "                dim_feedforward=self.feedforward_dim,\n",
    "                activation=F.gelu,\n",
    "                batch_first=True,\n",
    "                dropout=0.0,\n",
    "            ),\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ClassificationViT(nn.Module):\n",
    "    \"\"\"Vision transformer for classfication\n",
    "    Args:\n",
    "        n_classes: number of classes\n",
    "        embedding_dim: dimension of embedding\n",
    "        patch_size: side length of a square-shaped image patch\n",
    "        num_patches: number of image patches per height/width\n",
    "    Returns:\n",
    "        Logits of classfication\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, embedding_dim=256, patch_size=4, num_patches=8, num_channels=3, n_layers=4, feedforward_dim=1024):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_tokens = num_patches ** 2\n",
    "\n",
    "        self.transformer = Transformer(embedding_dim, n_layers=n_layers, feedforward_dim=feedforward_dim)\n",
    "        # We use random position encoding.\n",
    "        # You are welcome to try other methods, such as sinusoidal encoding.\n",
    "        self.position_encoding = nn.Parameter(\n",
    "            torch.randn(1, self.n_tokens, embedding_dim) * 0.02\n",
    "        )\n",
    "        self.patch_projection = nn.Linear((patch_size ** 2) * num_channels, embedding_dim)\n",
    "\n",
    "        # Multiheaded Attention Pooling for classification\n",
    "        self.cls_query = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                     num_heads=self.transformer.n_heads,\n",
    "                                                     batch_first=True)\n",
    "\n",
    "        # A Layernorm and a Linear layer are applied on ViT encoder embeddings\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        (1) Splitting images into fixed-size patches, dividing both height and width by patch size;\n",
    "        (2) Linearly embed each image patch, append CLS token;\n",
    "        (3) Add position embeddings;\n",
    "        (4) Feed the resulting sequence of vectors to Transformer encoder.\n",
    "        (5) Perform global average pooling.\n",
    "        (6) Apply output head to the embeddings to obtain the logits\n",
    "\n",
    "        Args:\n",
    "          images: (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        # BEGIN YOUR CODE\n",
    "\n",
    "        # (1) Split images into patches using patchify, can be implemented in 1 line\n",
    "\n",
    "        # (2) Linearly embed each patch, can be implemented in 1 line\n",
    "\n",
    "        # (3) Add position embeddings to the embedded patch in step (2), can be implemented in 1 line\n",
    "\n",
    "        # (4) Feed the resulting sequence of vectors to Transformer encoder, can be implemented in 1 line\n",
    "\n",
    "        # (5) Apply Multiheaded Attention Pooling, which applies self.cross_attention\n",
    "        # on cls_query and encoder output from (4)\n",
    "        # Can be implemented in < 5 lines\n",
    "\n",
    "        # (6) Apply output head to the output to obtain the logits, can be implemented in 1 line\n",
    "\n",
    "        return logits\n",
    "        # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T04:54:52.898437Z",
     "iopub.status.busy": "2024-09-17T04:54:52.897736Z",
     "iopub.status.idle": "2024-09-17T04:54:53.248865Z",
     "shell.execute_reply": "2024-09-17T04:54:53.247595Z",
     "shell.execute_reply.started": "2024-09-17T04:54:52.898398Z"
    },
    "id": "D3-wRAClbpnf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation\n",
    "model = ClassificationViT(10)\n",
    "model.load_state_dict(test_data['weights']['ClassificationViT'])\n",
    "x = test_data['input']['ClassificationViT.forward']\n",
    "y = model.forward(x)\n",
    "check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n",
    "\n",
    "model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n",
    "x = auto_grader_data['input']['ClassificationViT.forward']\n",
    "y = model.forward(x)\n",
    "auto_grader_data['output']['ClassificationViT.forward'] = y\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1m2Q5dNnZXT"
   },
   "source": [
    "### Data Loader and Preprocess\n",
    "\n",
    "We use ```torchvision``` to download and prepare images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:55:00.070499Z",
     "iopub.status.busy": "2024-09-17T04:55:00.069527Z",
     "iopub.status.idle": "2024-09-17T04:55:12.245697Z",
     "shell.execute_reply": "2024-09-17T04:55:12.244898Z",
     "shell.execute_reply.started": "2024-09-17T04:55:00.070448Z"
    },
    "id": "F_EBBMgv-FRI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/content/data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/content/data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnTEdan3r3_K"
   },
   "source": [
    "### Supervised Training ViT [train]\n",
    "\n",
    "We have obtained up to 73.5% validation accuracy. You are required to reach at least 65% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:55:12.248013Z",
     "iopub.status.busy": "2024-09-17T04:55:12.247338Z",
     "iopub.status.idle": "2024-09-17T04:58:36.173916Z",
     "shell.execute_reply": "2024-09-17T04:58:36.172831Z",
     "shell.execute_reply.started": "2024-09-17T04:55:12.247970Z"
    },
    "id": "l5SBXcv5t47u",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize model (ClassificationViT)\n",
    "# GPU(full): 6 mins (~30 secs per epoch)\n",
    "# CPU(full): 220 mins (~22 mins per epoch))\n",
    "# CPU(tiny): 10 mins (~1 min per epoch)\n",
    "print(f\"Creating ClassificationViT with MODEL_SIZE={MODEL_SIZE}\")\n",
    "if MODEL_SIZE == ModelSize.FULL:\n",
    "    model = ClassificationViT(10)\n",
    "else:\n",
    "    # TINY\n",
    "    model = ClassificationViT(10, embedding_dim=4, n_layers=1, feedforward_dim=4)\n",
    "\n",
    "# Move model to target device (eg GPU if available)\n",
    "model.to(torch_device)\n",
    "# Create optimizer for the model\n",
    "\n",
    "# You may want to tune these hyperparameters to get better performance.\n",
    "# Tip: you should be able to pass the autograder tests with these given \n",
    "#   hyperparameter values on the \"full\" model size, assuming your model \n",
    "#   implementations are correct.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
    "\n",
    "total_steps = 0\n",
    "num_epochs = 10\n",
    "train_logfreq = 100\n",
    "losses = []\n",
    "train_acc = []\n",
    "all_val_acc = []\n",
    "best_val_acc = 0\n",
    "\n",
    "savedir_path = './saved_models'\n",
    "os.makedirs(savedir_path,exist_ok=True)\n",
    "\n",
    "epoch_iterator = trange(num_epochs)\n",
    "for epoch in epoch_iterator:\n",
    "    # Train\n",
    "    model.train()\n",
    "    data_iterator = tqdm(trainloader)\n",
    "    for x, y in data_iterator:\n",
    "        total_steps += 1\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        logits = model(x)\n",
    "        loss = torch.mean(F.cross_entropy(logits, y))\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
    "\n",
    "        if total_steps % train_logfreq == 0:\n",
    "            losses.append(loss.item())\n",
    "            train_acc.append(accuracy.item())\n",
    "\n",
    "    # Validation\n",
    "    val_acc = []\n",
    "    model.eval()\n",
    "    for x, y in testloader:\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        val_acc.append(accuracy.item())\n",
    "\n",
    "    mean_val_acc = np.mean(val_acc)\n",
    "    all_val_acc.append(mean_val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if mean_val_acc > best_val_acc:\n",
    "        best_val_acc = mean_val_acc\n",
    "        torch.save(model.state_dict(), f'{savedir_path}/vit.pth')\n",
    "\n",
    "    epoch_iterator.set_postfix(val_acc=mean_val_acc, best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:58:55.403423Z",
     "iopub.status.busy": "2024-09-17T04:58:55.402417Z",
     "iopub.status.idle": "2024-09-17T04:58:56.508626Z",
     "shell.execute_reply": "2024-09-17T04:58:56.507649Z",
     "shell.execute_reply.started": "2024-09-17T04:58:55.403378Z"
    },
    "id": "8lxXhc66t47u",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axd = plt.subplot_mosaic([['loss', 'train_acc', 'val_acc']], figsize=(15, 5))\n",
    "\n",
    "axd['loss'].plot(losses)\n",
    "axd['loss'].set_title('Train Loss')\n",
    "\n",
    "axd['train_acc'].plot(train_acc)\n",
    "axd['train_acc'].set_title('Train Accuracy')\n",
    "\n",
    "axd['val_acc'].plot(all_val_acc)\n",
    "axd['val_acc'].set_title('Val Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T03:13:03.832304Z",
     "iopub.status.busy": "2024-09-10T03:13:03.831604Z",
     "iopub.status.idle": "2024-09-10T03:13:03.838857Z",
     "shell.execute_reply": "2024-09-10T03:13:03.838003Z",
     "shell.execute_reply.started": "2024-09-10T03:13:03.832256Z"
    },
    "id": "OikMO6_LCmik",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation, your accuracy should be greater than 65%\n",
    "auto_grader_data['output']['vit_acc'] = best_val_acc\n",
    "save_auto_grader_data()\n",
    "check_acc(best_val_acc, threshold=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij4IVDMIsB5o"
   },
   "source": [
    "## Masked AutoEncoder\n",
    "\n",
    "The second part of this notebook is implementing [Masked Autoencoder (MAE)](https://en.wikipedia.org/wiki/Masked_autoencoder).\n",
    "The idea of MAE is masking random patches of the input image and reconstruct the missing pixels. This whole achitecture can be seen in the following figure.\n",
    "![mae](https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png)\n",
    "\n",
    "You will train MAE without labels on CIFAR, aka, self-supervised learning.\n",
    "Then you will use the self-supervised pretrained model for linear classification and finetuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:59:02.758748Z",
     "iopub.status.busy": "2024-09-17T04:59:02.758013Z",
     "iopub.status.idle": "2024-09-17T04:59:02.772349Z",
     "shell.execute_reply": "2024-09-17T04:59:02.771428Z",
     "shell.execute_reply.started": "2024-09-17T04:59:02.758709Z"
    },
    "id": "eKkRHTjMUOJ7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def index_sequence(x, ids):\n",
    "    \"\"\"Index tensor (x) with indices given by ids\n",
    "    Args:\n",
    "        x: input sequence tensor, can be 2D (batch x length) or 3D (batch x length x feature)\n",
    "        ids: 2D indices (batch x length) for re-indexing the sequence tensor\n",
    "    Returns:\n",
    "        result: a tensor with the same shape as x, but permuted on the \"length\" dimension according to ids.\n",
    "    \"\"\"\n",
    "    batch, length = ids.shape\n",
    "\n",
    "    assert len(x.shape) in [2, 3], f'Input tensor x should have 2 or 3 dimensions, but has shape {x.shape}'\n",
    "    if len(x.shape) == 3:\n",
    "        ids = ids.unsqueeze(-1).expand(-1, -1, x.shape[-1])\n",
    "    result = torch.take_along_dim(x, ids, dim=1)\n",
    "    assert result.shape == x.shape\n",
    "    return result\n",
    "\n",
    "def random_masking(x, keep_length, ids_shuffle):\n",
    "    \"\"\"Apply random masking on input tensor\n",
    "    Args:\n",
    "        x:  (batch, length, feature) input patches\n",
    "        keep_length: int, length of unmasked patches\n",
    "        ids_shuffle: (batch, length) random indices for shuffling the input sequence\n",
    "    Returns:\n",
    "        kept: (batch, keep_length, feature) un-masked part of x\n",
    "        mask: (batch, length) a 2D (batch x length) mask tensor of 0s and 1s indicated which\n",
    "            part of x is masked out. The value 0 indicates not masked and 1\n",
    "            indicates masked.\n",
    "        ids_restore: (batch, length) indices to restore x. If we take the kept part and masked\n",
    "            part of x, concatentate them together and index it with ids_restore,\n",
    "            we should get x back.\n",
    "\n",
    "    Hint:\n",
    "        ids_shuffle contains the indices used to shuffle the sequence (patches).\n",
    "        You should use the provided index_sequence function to re-index the\n",
    "        sequence, and keep the first keep_length number of patches.\n",
    "    \"\"\"\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    # (1) Shuffle the sequence using index_sequence\n",
    "\n",
    "    # (2) Keep the first keep_length patches\n",
    "\n",
    "    # (3) Create ids_restore. For reference see the argsort permutation trick\n",
    "    #  https://arogozhnikov.github.io/2015/09/29/NumpyTipsAndTricks1.html\n",
    "\n",
    "    # (4) Create the mask, can be implemented in <5 lines\n",
    "\n",
    "    return kept, mask, ids_restore\n",
    "    # END YOUR CODE\n",
    "\n",
    "def restore_masked(kept_x, masked_x, ids_restore):\n",
    "    \"\"\"Restore masked patches\n",
    "    Args:\n",
    "        kept_x: unmasked patches, (batch x keep_length x feature)\n",
    "        masked_x: masked patches, (batch x mask_length x feature)\n",
    "        ids_restore: indices to restore x, (batch x length)\n",
    "    Returns:\n",
    "        restored: restored patches, (batch x length x feature)\n",
    "    \"\"\"\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    # (1) Concatenate kept and masked patches\n",
    "\n",
    "    # (2) Restore the original order\n",
    "\n",
    "    return restored\n",
    "    # END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufTgPU_Rszsd"
   },
   "source": [
    "### Random Masking and Restore\n",
    "\n",
    "To get started with MAE, you need to implement ```random_masking``` to mask random patches from the input image and ```restore_masked``` to combine reconstructed masked part and unmasked part to restore the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T04:59:04.778420Z",
     "iopub.status.busy": "2024-09-17T04:59:04.778032Z",
     "iopub.status.idle": "2024-09-17T04:59:04.802671Z",
     "shell.execute_reply": "2024-09-17T04:59:04.801807Z",
     "shell.execute_reply.started": "2024-09-17T04:59:04.778384Z"
    },
    "id": "QxXqQ6Qk9FLX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation\n",
    "x, ids_shuffle = test_data['input']['random_masking']\n",
    "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
    "kept_t, mask_t, ids_restore_t = test_data['output']['random_masking']\n",
    "check_error('random_masking: kept', kept, kept_t)\n",
    "check_error('random_masking: mask', mask, mask_t)\n",
    "check_error('random_masking: ids_restore', ids_restore, ids_restore_t)\n",
    "\n",
    "x, ids_shuffle = auto_grader_data['input']['random_masking']\n",
    "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
    "auto_grader_data['output']['random_masking'] = (kept, mask, ids_restore)\n",
    "save_auto_grader_data()\n",
    "\n",
    "kept_x, masked_x, ids_restore = test_data['input']['restore_masked']\n",
    "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
    "check_error('restore_masked', restored, test_data['output']['restore_masked'])\n",
    "\n",
    "kept_x, masked_x, ids_restore = auto_grader_data['input']['restore_masked']\n",
    "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
    "auto_grader_data['output']['restore_masked'] = (kept, mask, ids_restore)\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0NGq77mt47v"
   },
   "source": [
    "## Implement MAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T04:59:07.924849Z",
     "iopub.status.busy": "2024-09-17T04:59:07.923993Z",
     "iopub.status.idle": "2024-09-17T04:59:07.945345Z",
     "shell.execute_reply": "2024-09-17T04:59:07.944330Z",
     "shell.execute_reply.started": "2024-09-17T04:59:07.924811Z"
    },
    "id": "9ud7I0SXK0mk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "class MaskedAutoEncoder(nn.Module):\n",
    "    \"\"\"MAE Encoder\n",
    "    Args:\n",
    "        encoder: ViT encoder\n",
    "        decoder: ViT decoder\n",
    "        encoder_embedding_dim: embedding size of encoder\n",
    "        decoder_embedding_dim: embedding size of decoder\n",
    "        patch_size: side length of a square-shaped image patch\n",
    "        num_patches: number of patches per height/width\n",
    "        mask_ratio: percentage of masked patches\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, encoder_embedding_dim=256,\n",
    "                 decoder_embedding_dim=128, patch_size=4, num_patches=8,\n",
    "                 mask_ratio=0.75, channels=3):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.channels = channels\n",
    "\n",
    "        self.masked_length = int(num_patches * num_patches * mask_ratio)\n",
    "        self.keep_length = num_patches * num_patches - self.masked_length\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.encoder_input_projection = nn.Linear(patch_size * patch_size * channels, encoder_embedding_dim)\n",
    "        self.decoder_input_projection = nn.Linear(encoder_embedding_dim, decoder_embedding_dim)\n",
    "        self.decoder_output_projection = nn.Linear(decoder_embedding_dim, patch_size * patch_size * channels)\n",
    "        self.encoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, encoder_embedding_dim) * 0.02)\n",
    "        self.decoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, decoder_embedding_dim) * 0.02)\n",
    "        self.masked_tokens = nn.Parameter(torch.randn(1, 1, decoder_embedding_dim) * 0.02)\n",
    "\n",
    "    def forward_encoder(self, images, ids_shuffle=None):\n",
    "        \"\"\"Encode input images\n",
    "        You should implement the following steps\n",
    "        (1) patchify images into patches\n",
    "        (2) linear projection\n",
    "        (3) add position encoding\n",
    "        (4) apply random masking, and pass it to ViT encoder\n",
    "        Args:\n",
    "            images: (batch x height x width x channel)\n",
    "            ids_shuffle: (batch x num_patches) tensor for shuffling the patches.\n",
    "                         If None, then a random shuffle is used.\n",
    "        Returns:\n",
    "            encoder_embeddings: output patches, (batch x keep_length x feature)\n",
    "            mask: a 2D (batch x num_patches) mask tensor of 0s and 1s indicated which\n",
    "                  part of x is masked out. The value 0 indicates not masked and 1\n",
    "                  indicates masked.\n",
    "            ids_restore: (batch x num_patches) tensor, indices to restore x.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = images.shape\n",
    "        assert height == width == self.num_patches * self.patch_size\n",
    "\n",
    "        # Generate random shuffling indices\n",
    "        if ids_shuffle is None:\n",
    "            ids_shuffle = torch.argsort(\n",
    "                torch.rand(\n",
    "                    (batch_size, self.num_patches * self.num_patches),\n",
    "                    device=images.device\n",
    "                ),\n",
    "                dim=1\n",
    "            )\n",
    "        # BEGIN YOUR CODE\n",
    "        # (1) Patchify images\n",
    "\n",
    "        # (2) Linear projection\n",
    "\n",
    "        # (3) Add position encoding to linear projections\n",
    "\n",
    "        # (4) Apply random masking and pass to ViT encoder\n",
    "\n",
    "        # (5) Pass through the encoder\n",
    "\n",
    "        return encoder_embeddings, mask, ids_restore\n",
    "        # END YOUR CODE\n",
    "\n",
    "    def forward_decoder(self, encoder_embeddings, ids_restore):\n",
    "        \"\"\"Decode encoder embeddings\n",
    "        You should implement the following steps\n",
    "        (1) linear projection of encoder embeddings\n",
    "        (2) restore array of patches from masked_patches and encoder predictions\n",
    "        (3) add position encoding\n",
    "        (4) pass it to ViT decoder, then apply the output projection to predict image patches\n",
    "\n",
    "        Returns:\n",
    "            predicted_patches: (batch, num_patches, patch_size * patch_size * channels)\n",
    "                               All predicted patches, including masked and unmasked ones.\n",
    "        \"\"\"\n",
    "        # BEGIN YOUR CODE\n",
    "        batch_size = encoder_embeddings.shape[0]\n",
    "\n",
    "        # (1) Linear projection of encoder embeddings\n",
    "\n",
    "        # (2) Restore sequence from masked_patches and encoder predictions. Can be implemented in <5 lines\n",
    "\n",
    "        # (3) Add position encoding to result\n",
    "\n",
    "        # (4) Pass through ViT decoder and apply output projection\n",
    "\n",
    "        return predicted_patches\n",
    "        # END YOUR CODE\n",
    "\n",
    "    def forward(self, images):\n",
    "        encoder_output, mask, ids_restore = self.forward_encoder(images)\n",
    "        decoder_output = self.forward_decoder(encoder_output, ids_restore)\n",
    "        return decoder_output, mask\n",
    "\n",
    "    def forward_encoder_representation(self, images):\n",
    "        \"\"\"Encode images without applying random masking to get representation\n",
    "        of input images. It is just forward_encoder without masking.\n",
    "\n",
    "        Returns:\n",
    "            encoder_embeddings: output patches, (batch x keep_length x feature)\n",
    "        \"\"\"\n",
    "        # BEGIN YOUR CODE\n",
    "\n",
    "        # (1) Patchify images\n",
    "\n",
    "        # (2) Apply linear input projection and add position encoding\n",
    "\n",
    "        # (3) Pass through the encoder\n",
    "        return encoder_embeddings\n",
    "        # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-10T01:56:01.453196Z",
     "iopub.status.busy": "2024-09-10T01:56:01.452821Z",
     "iopub.status.idle": "2024-09-10T01:56:01.552929Z",
     "shell.execute_reply": "2024-09-10T01:56:01.551550Z",
     "shell.execute_reply.started": "2024-09-10T01:56:01.453157Z"
    },
    "id": "obJhGP5vfaBl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation\n",
    "model = MaskedAutoEncoder(\n",
    "    Transformer(embedding_dim=256, n_layers=4),\n",
    "    Transformer(embedding_dim=128, n_layers=2),\n",
    ")\n",
    "\n",
    "model.load_state_dict(test_data['weights']['MaskedAutoEncoder'])\n",
    "images, ids_shuffle = test_data['input']['MaskedAutoEncoder.forward_encoder']\n",
    "encoder_embeddings_t, mask_t, ids_restore_t = test_data['output']['MaskedAutoEncoder.forward_encoder']\n",
    "encoder_embeddings, mask, ids_restore = model.forward_encoder(\n",
    "    images, ids_shuffle\n",
    ")\n",
    "\n",
    "check_error(\n",
    "    'MaskedAutoEncoder.forward_encoder: encoder_embeddings',\n",
    "    encoder_embeddings, encoder_embeddings_t\n",
    ")\n",
    "check_error(\n",
    "    'MaskedAutoEncoder.forward_encoder: mask',\n",
    "    mask, mask_t\n",
    ")\n",
    "check_error(\n",
    "    'MaskedAutoEncoder.forward_encoder: ids_restore',\n",
    "    ids_restore, ids_restore_t\n",
    ")\n",
    "\n",
    "encoder_embeddings, ids_restore = test_data['input']['MaskedAutoEncoder.forward_decoder']\n",
    "decoder_output_t = test_data['output']['MaskedAutoEncoder.forward_decoder']\n",
    "decoder_output = model.forward_decoder(encoder_embeddings, ids_restore)\n",
    "check_error(\n",
    "    'MaskedAutoEncoder.forward_decoder',\n",
    "    decoder_output,\n",
    "    decoder_output_t\n",
    ")\n",
    "\n",
    "images = test_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
    "encoder_representations_t = test_data['output']['MaskedAutoEncoder.forward_encoder_representation']\n",
    "encoder_representations = model.forward_encoder_representation(images)\n",
    "check_error(\n",
    "    'MaskedAutoEncoder.forward_encoder_representation',\n",
    "    encoder_representations,\n",
    "    encoder_representations_t\n",
    ")\n",
    "\n",
    "model = MaskedAutoEncoder(\n",
    "    Transformer(embedding_dim=256, n_layers=4),\n",
    "    Transformer(embedding_dim=128, n_layers=2),\n",
    ")\n",
    "\n",
    "model.load_state_dict(auto_grader_data['weights']['MaskedAutoEncoder'])\n",
    "images, ids_shuffle = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder']\n",
    "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder'] = model.forward_encoder(\n",
    "    images, ids_shuffle\n",
    ")\n",
    "\n",
    "encoder_embeddings, ids_restore = auto_grader_data['input']['MaskedAutoEncoder.forward_decoder']\n",
    "auto_grader_data['output']['MaskedAutoEncoder.forward_decoder'] = model.forward_decoder(encoder_embeddings, ids_restore)\n",
    "\n",
    "images = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
    "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder_representation'] = model.forward_encoder_representation(images)\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KO8ZzAOyJ_"
   },
   "source": [
    "### Train Masked Autoencoder [train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:04:11.027843Z",
     "iopub.status.busy": "2024-09-17T05:04:11.027511Z",
     "iopub.status.idle": "2024-09-17T05:09:03.739020Z",
     "shell.execute_reply": "2024-09-17T05:09:03.737923Z",
     "shell.execute_reply.started": "2024-09-17T05:04:11.027810Z"
    },
    "id": "-STAeJLbt47w",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize MAE model\n",
    "# GPU(full): 10 mins (30 secs per epoch)\n",
    "# CPU(full): 240 mins (12 mins per epoch)\n",
    "# CPU(tiny): 20 mins (1 min per epoch)\n",
    "print(f\"Creating MaskedAutoEncoder with MODEL_SIZE={MODEL_SIZE}\")\n",
    "if MODEL_SIZE == ModelSize.FULL:\n",
    "    model = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=256, n_layers=4),\n",
    "        Transformer(embedding_dim=128, n_layers=2),\n",
    "    )\n",
    "else:\n",
    "    # TINY\n",
    "    model = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=4, n_layers=1, feedforward_dim=4),\n",
    "        Transformer(embedding_dim=4, n_layers=1, feedforward_dim=4),\n",
    "        encoder_embedding_dim=4,\n",
    "        decoder_embedding_dim=4,\n",
    "    )\n",
    "\n",
    "# Move the model to target device (eg GPU if available)\n",
    "model.to(torch_device)\n",
    "# Create optimizer\n",
    "\n",
    "# You may want to tune these hyperparameters to get better performance\n",
    "# Tip: with correct model implementation (and on \"full\" model size), you should be able\n",
    "#   to pass autograder tests with these given hyperparameter values.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=0.05)\n",
    "\n",
    "total_steps = 0\n",
    "num_epochs = 20\n",
    "train_logfreq = 100\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "savedir_path = './saved_models'\n",
    "os.makedirs(savedir_path,exist_ok=True)\n",
    "\n",
    "epoch_iterator = trange(num_epochs)\n",
    "for epoch in epoch_iterator:\n",
    "    # Train\n",
    "    data_iterator = tqdm(trainloader)\n",
    "    for x, y in data_iterator:\n",
    "        total_steps += 1\n",
    "        x = x.to(torch_device)\n",
    "        image_patches = patchify(x)\n",
    "        predicted_patches, mask = model(x)\n",
    "        loss = torch.sum(torch.mean(torch.square(image_patches - predicted_patches), dim=-1) * mask) / mask.sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_iterator.set_postfix(loss=loss.item())\n",
    "        if total_steps % train_logfreq == 0:\n",
    "            losses.append(loss.item())\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                torch.save(model.state_dict(), f'{savedir_path}/mae_pretrained.pth')\n",
    "\n",
    "# load trained model\n",
    "model.load_state_dict(torch.load(f'{savedir_path}/mae_pretrained.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:12:36.061068Z",
     "iopub.status.busy": "2024-09-17T05:12:36.060666Z",
     "iopub.status.idle": "2024-09-17T05:12:36.423168Z",
     "shell.execute_reply": "2024-09-17T05:12:36.422080Z",
     "shell.execute_reply.started": "2024-09-17T05:12:36.061030Z"
    },
    "id": "n3tKIB2Tt47w",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = np.linspace(0, 1, len(losses)) * len(epoch_iterator)\n",
    "plt.plot(epochs, losses)\n",
    "plt.title('MAE Train Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UV8dPF8Pw4k"
   },
   "source": [
    "### Use pretrained MAE model for classification\n",
    "\n",
    "We train a shallow head to classify on top of the self-supervised representation learned by the MAE. It first processes the output from the MAE encoder by a multiheaded attention pooling, then layernorm, then linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:12:44.910279Z",
     "iopub.status.busy": "2024-09-17T05:12:44.909897Z",
     "iopub.status.idle": "2024-09-17T05:12:44.920119Z",
     "shell.execute_reply": "2024-09-17T05:12:44.919063Z",
     "shell.execute_reply.started": "2024-09-17T05:12:44.910243Z"
    },
    "id": "QODCykcJS0ec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClassificationMAE(nn.Module):\n",
    "    \"\"\"MAP-layernorm-linear classifier trained on the\n",
    "       self-supervised representations learned by MAE.\n",
    "    Args:\n",
    "        n_classes: number of classes\n",
    "        mae: mae model\n",
    "        embedding_dim: embedding dimension of mae output\n",
    "        detach: if True, only the classification head is updated.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, mae, embedding_dim=256, detach=False):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mae = mae\n",
    "\n",
    "        # Multiheaded Attention Pooling for classification\n",
    "        self.cls_query = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                     num_heads=4,\n",
    "                                                     batch_first=True)\n",
    "\n",
    "        # A Layernorm and a Linear layer are applied on ViT encoder embeddings\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        When self.detach=True, use linear classification, when self.detach=False,\n",
    "        use full finetuning.\n",
    "        \"\"\"\n",
    "        self.detach = detach\n",
    "\n",
    "    def forward(self, images):\n",
    "        # BEGIN YOUR CODE\n",
    "\n",
    "        # (1) Get encoder representations using MAE\n",
    "\n",
    "        # (2) If detach, detach the output (https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html)\n",
    "\n",
    "        # (3) Apply Multiheaded Attention Pooling, can be implemented in <5 lines\n",
    "\n",
    "        # (4) Reshape pooled output\n",
    "\n",
    "        # (5) Apply output head to get logits\n",
    "\n",
    "        return logits\n",
    "        # END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T05:09:04.117036Z",
     "iopub.status.busy": "2024-09-17T05:09:04.116653Z",
     "iopub.status.idle": "2024-09-17T05:09:04.229006Z",
     "shell.execute_reply": "2024-09-17T05:09:04.227496Z",
     "shell.execute_reply.started": "2024-09-17T05:09:04.116993Z"
    },
    "id": "Fi07CiNMt-pX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation\n",
    "model = ClassificationMAE(\n",
    "    10,\n",
    "    MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=256, n_layers=4),\n",
    "        Transformer(embedding_dim=128, n_layers=2),\n",
    "    )\n",
    ")\n",
    "\n",
    "model.load_state_dict(test_data['weights']['ClassificationMAE'])\n",
    "\n",
    "check_error(\n",
    "    'ClassificationMAE.forward',\n",
    "    model(test_data['input']['ClassificationMAE.forward']),\n",
    "    test_data['output']['ClassificationMAE.forward']\n",
    ")\n",
    "\n",
    "model = ClassificationMAE(\n",
    "    10,\n",
    "    MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=256, n_layers=4),\n",
    "        Transformer(embedding_dim=128, n_layers=2),\n",
    "    )\n",
    ")\n",
    "\n",
    "model.load_state_dict(auto_grader_data['weights']['ClassificationMAE'])\n",
    "auto_grader_data['output']['ClassificationMAE.forward'] = model(\n",
    "    auto_grader_data['input']['ClassificationMAE.forward']\n",
    ")\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyLwLC6VPz2Y"
   },
   "source": [
    "### Linear Classification [train]\n",
    "\n",
    "A linear classifier is trained on self-supervised representations learned by MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:12:50.563609Z",
     "iopub.status.busy": "2024-09-17T05:12:50.562727Z",
     "iopub.status.idle": "2024-09-17T05:18:01.859625Z",
     "shell.execute_reply": "2024-09-17T05:18:01.858461Z",
     "shell.execute_reply.started": "2024-09-17T05:12:50.563568Z"
    },
    "id": "KdPfl0MJrfng",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initilize classification model; set detach=True to only update the linear classifier.\n",
    "# GPU(full): 10 mins (30 secs per epoch)\n",
    "# CPU(full): >3 hours\n",
    "# CPU(tiny): 13 mins (40 secs per epoch)\n",
    "# IMPORTANT: `mae` must be kept in sync with the model arch used to generate \"mae_pretrained.pth\"\n",
    "#   If you get an error like this:\n",
    "#       Error(s) in loading state_dict for MaskedAutoEncoder: Missing key(s) in state_dict\n",
    "#   Then this means there is a mismatch between your `mae` definition and what\n",
    "#   is saved in \"mae_pretrained.pth\".\n",
    "#   Worst case: if you don't remember the settings, you can re-run the \"Train Masked Autoencoder\",\n",
    "#     then copy+paste the model definition from that cell to here\n",
    "print(f\"Creating MaskedAutoEncoder with MODEL_SIZE={MODEL_SIZE}\")\n",
    "if MODEL_SIZE == ModelSize.FULL:\n",
    "    encoder_embedding_dim = 256\n",
    "    mae = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=encoder_embedding_dim, n_layers=4),\n",
    "        Transformer(embedding_dim=128, n_layers=2),\n",
    "    )\n",
    "else:\n",
    "    # TINY\n",
    "    encoder_embedding_dim = 4\n",
    "    mae = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=encoder_embedding_dim, n_layers=1, feedforward_dim=4),\n",
    "        Transformer(embedding_dim=4, n_layers=1, feedforward_dim=4),\n",
    "        encoder_embedding_dim=encoder_embedding_dim,\n",
    "        decoder_embedding_dim=4,\n",
    "    )\n",
    "\n",
    "mae.load_state_dict(torch.load(f'{savedir_path}/mae_pretrained.pth', weights_only=True))\n",
    "model = ClassificationMAE(10, mae, embedding_dim=encoder_embedding_dim, detach=True)\n",
    "model.to(torch_device)\n",
    "\n",
    "# You may want to tune these hyperparameters to get better performance\n",
    "# Tip: with a correct implementation (and \"full\" model size), you should be able to pass\n",
    "#   the autograder tests with these provided hyperparameter values.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
    "\n",
    "total_steps = 0\n",
    "num_epochs = 20\n",
    "train_logfreq = 100\n",
    "losses = []\n",
    "train_acc = []\n",
    "all_val_acc = []\n",
    "best_val_acc = 0\n",
    "\n",
    "epoch_iterator = trange(num_epochs)\n",
    "for epoch in epoch_iterator:\n",
    "    # Train\n",
    "    data_iterator = tqdm(trainloader)\n",
    "    for x, y in data_iterator:\n",
    "        total_steps += 1\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        logits = model(x)\n",
    "        loss = torch.mean(F.cross_entropy(logits, y))\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
    "\n",
    "        if total_steps % train_logfreq == 0:\n",
    "            losses.append(loss.item())\n",
    "            train_acc.append(accuracy.item())\n",
    "\n",
    "    # Validation\n",
    "    val_acc = []\n",
    "    model.eval()\n",
    "    for x, y in testloader:\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        with torch.no_grad():\n",
    "          logits = model(x)\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        val_acc.append(accuracy.item())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    all_val_acc.append(np.mean(val_acc))\n",
    "\n",
    "    # Save best model\n",
    "    if np.mean(val_acc) > best_val_acc:\n",
    "        best_val_acc = np.mean(val_acc)\n",
    "        torch.save(model.state_dict(), f'{savedir_path}/mae_cls_head.pth')\n",
    "\n",
    "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:18:01.862101Z",
     "iopub.status.busy": "2024-09-17T05:18:01.861744Z",
     "iopub.status.idle": "2024-09-17T05:18:03.214402Z",
     "shell.execute_reply": "2024-09-17T05:18:03.213450Z",
     "shell.execute_reply.started": "2024-09-17T05:18:01.862067Z"
    },
    "id": "IFHaqNzet471",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "epochs = np.linspace(0, 1, len(losses)) * len(epoch_iterator)\n",
    "\n",
    "fig, axd = plt.subplot_mosaic([['loss', 'train_acc', 'val_acc']], figsize=(15, 5))\n",
    "\n",
    "axd['loss'].plot(epochs, losses)\n",
    "axd['loss'].set_title('Train Loss')\n",
    "\n",
    "axd['train_acc'].plot(epochs, train_acc)\n",
    "axd['train_acc'].set_title('Train Accuracy')\n",
    "\n",
    "axd['val_acc'].plot(all_val_acc)\n",
    "axd['val_acc'].set_title('Val Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T05:18:03.215860Z",
     "iopub.status.busy": "2024-09-17T05:18:03.215548Z",
     "iopub.status.idle": "2024-09-17T05:18:03.222660Z",
     "shell.execute_reply": "2024-09-17T05:18:03.221634Z",
     "shell.execute_reply.started": "2024-09-17T05:18:03.215827Z"
    },
    "id": "RwXyr42cDYr3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation, your accuracy should be greater than 24%\n",
    "auto_grader_data['output']['mae_linear_acc'] = best_val_acc\n",
    "save_auto_grader_data()\n",
    "check_acc(best_val_acc, threshold=0.24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frkTBj1wQo_R"
   },
   "source": [
    "### Full Finetuning [train]\n",
    "\n",
    "A linear classifer and the pretrained MAE model are jointly updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:18:03.225065Z",
     "iopub.status.busy": "2024-09-17T05:18:03.224750Z",
     "iopub.status.idle": "2024-09-17T05:24:49.975251Z",
     "shell.execute_reply": "2024-09-17T05:24:49.974144Z",
     "shell.execute_reply.started": "2024-09-17T05:18:03.225029Z"
    },
    "id": "dGTMdBgci8PW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize classification model; set detach=False to update both the linear classifier and pretrained MAE model.\n",
    "# GPU(T4): 11 mins (33 secs per epoch)\n",
    "# CPU: >3 hours\n",
    "# CPU(tiny): 18 mins (50 secs per epoch)\n",
    "\n",
    "# IMPORTANT: `mae` must be kept in sync with the model arch used to generate \"mae_pretrained.pth\"\n",
    "print(f\"Creating MaskedAutoEncoder with MODEL_SIZE={MODEL_SIZE}\")\n",
    "if MODEL_SIZE == ModelSize.FULL:\n",
    "    encoder_embedding_dim = 256\n",
    "    mae = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=encoder_embedding_dim, n_layers=4),\n",
    "        Transformer(embedding_dim=128, n_layers=2),\n",
    "    )\n",
    "else:\n",
    "    # TINY\n",
    "    encoder_embedding_dim = 4\n",
    "    mae = MaskedAutoEncoder(\n",
    "        Transformer(embedding_dim=encoder_embedding_dim, n_layers=1, feedforward_dim=4),\n",
    "        Transformer(embedding_dim=4, n_layers=1, feedforward_dim=4),\n",
    "        encoder_embedding_dim=encoder_embedding_dim,\n",
    "        decoder_embedding_dim=4,\n",
    "    )\n",
    "mae.load_state_dict(torch.load(f'{savedir_path}/mae_pretrained.pth', weights_only=True))\n",
    "model = ClassificationMAE(10, mae, embedding_dim=encoder_embedding_dim, detach=False)\n",
    "model.to(torch_device)\n",
    "\n",
    "# You may want to tune these hyperparameters to get better performance\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
    "\n",
    "total_steps = 0\n",
    "num_epochs = 20\n",
    "train_logfreq = 100\n",
    "losses = []\n",
    "train_acc = []\n",
    "all_val_acc = []\n",
    "best_val_acc = 0\n",
    "\n",
    "epoch_iterator = trange(num_epochs)\n",
    "for epoch in epoch_iterator:\n",
    "    # Train\n",
    "    data_iterator = tqdm(trainloader)\n",
    "    for x, y in data_iterator:\n",
    "        total_steps += 1\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        logits = model(x)\n",
    "        loss = torch.mean(F.cross_entropy(logits, y))\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
    "\n",
    "        if total_steps % train_logfreq == 0:\n",
    "            losses.append(loss.item())\n",
    "            train_acc.append(accuracy.item())\n",
    "\n",
    "    # Validation\n",
    "    val_acc = []\n",
    "    model.eval()\n",
    "    for x, y in testloader:\n",
    "        x, y = x.to(torch_device), y.to(torch_device)\n",
    "        with torch.no_grad():\n",
    "          logits = model(x)\n",
    "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "        val_acc.append(accuracy.item())\n",
    "    model.train()\n",
    "\n",
    "    all_val_acc.append(np.mean(val_acc))\n",
    "\n",
    "    # Save best model\n",
    "    if np.mean(val_acc) > best_val_acc:\n",
    "        best_val_acc = np.mean(val_acc)\n",
    "        torch.save(model.state_dict(), f'{savedir_path}/mae_cls_full.pth')\n",
    "\n",
    "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T05:24:49.977278Z",
     "iopub.status.busy": "2024-09-17T05:24:49.976931Z",
     "iopub.status.idle": "2024-09-17T05:24:51.075037Z",
     "shell.execute_reply": "2024-09-17T05:24:51.074117Z",
     "shell.execute_reply.started": "2024-09-17T05:24:49.977244Z"
    },
    "id": "d4e6Fi02t472",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "epochs = np.linspace(0, 1, len(losses)) * len(epoch_iterator)\n",
    "\n",
    "fig, axd = plt.subplot_mosaic([['loss', 'train_acc', 'val_acc']], figsize=(15, 5))\n",
    "\n",
    "axd['loss'].plot(epochs, losses)\n",
    "axd['loss'].set_title('Train Loss')\n",
    "\n",
    "axd['train_acc'].plot(epochs, train_acc)\n",
    "axd['train_acc'].set_title('Train Accuracy')\n",
    "\n",
    "axd['val_acc'].plot(all_val_acc)\n",
    "axd['val_acc'].set_title('Val Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-17T05:24:51.076628Z",
     "iopub.status.busy": "2024-09-17T05:24:51.076272Z",
     "iopub.status.idle": "2024-09-17T05:24:51.084058Z",
     "shell.execute_reply": "2024-09-17T05:24:51.083202Z",
     "shell.execute_reply.started": "2024-09-17T05:24:51.076587Z"
    },
    "id": "Q9k0Ca7MDSFP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Test your implementation, your validation accuracy should be greater than 65%\n",
    "auto_grader_data['output']['mae_finetune_acc'] = best_val_acc\n",
    "save_auto_grader_data()\n",
    "check_acc(best_val_acc, threshold=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3i2FR3LQ7hz"
   },
   "source": [
    "## Prepare Gradescope submission\n",
    "\n",
    "**NOTE:** The following uses the value of `root_folder` that you set in the beginning. Make sure it's correct before running these cells!\n",
    "\n",
    "Run the following cell will automatically prepare and downloads ```hw4_submission.zip``` (to your browser's default download directory).\n",
    "\n",
    "Upload the downloaded file to Gradescope.\n",
    "The Gradescope will run an autograder on the files you submit.\n",
    "\n",
    "It is very unlikely but still possible that your implementation might fail to pass some test cases due to randomness.\n",
    "If you think your code is correct, you can simply rerun the autograder to check check whether it is really due to randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFzRCAcMBu5G"
   },
   "outputs": [],
   "source": [
    "# Sanity check: do these look correct? Notably, `root_folder` and \"!pwd\" should output the same thing\n",
    "print(\"root_folder:\")\n",
    "print(root_folder)\n",
    "print(\"pwd:\")\n",
    "!pwd\n",
    "print(\"ls:\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:07:59.676652Z",
     "iopub.status.busy": "2024-09-10T04:07:59.676176Z",
     "iopub.status.idle": "2024-09-10T04:08:07.883873Z",
     "shell.execute_reply": "2024-09-10T04:08:07.882610Z",
     "shell.execute_reply.started": "2024-09-10T04:07:59.676589Z"
    },
    "id": "Iws2RCSHt472",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the dictionaries\n",
    "save_auto_grader_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:13:16.391548Z",
     "iopub.status.busy": "2024-09-10T04:13:16.390908Z",
     "iopub.status.idle": "2024-09-10T04:13:17.484439Z",
     "shell.execute_reply": "2024-09-10T04:13:17.483096Z",
     "shell.execute_reply.started": "2024-09-10T04:13:16.391487Z"
    },
    "id": "YUx6idNXt473",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: be sure that `root_folder` is defined correctly in previous cell\n",
    "# Note: It's OK if the `rm` command fails, this is normal if this is your first\n",
    "#   time running this.\n",
    "os.chdir(root_folder)\n",
    "!pwd # make sure we are in the right dir\n",
    "!ls\n",
    "\n",
    "!rm hw4_submission.zip\n",
    "!zip hw4_submission.zip -r *.ipynb autograder.pt\n",
    "\n",
    "import datetime\n",
    "from google.colab import files\n",
    "# ex: '2024-11-25_06-34-33'\n",
    "cur_timestamp_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "outpath_zip = f\"hw4_submission_{cur_timestamp_str}.zip\"\n",
    "files.download(outpath_zip)\n",
    "print(f\"Finished downloading {outpath_zip}! Upload this zip file to Gradescope as your submission to run the autograder. The {outpath_zip} file will be in your browser's default download directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BbOcbn2t473"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1lLHAuIs4YW2tmG8Mhbc7VgvM7oiFev0E",
     "timestamp": 1725325510720
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
